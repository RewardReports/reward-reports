<!-- Author: Soham Mehta --> <!-- Description:  -->

# System Details

## System Owner

This system was developed by Meta AI, in partnership with ParlAI and Metaseq. According to the system’s blog post, ”This work was undertaken by a team
that includes Kurt Shuster, Jing Xu, Mojtaba Komeili,
Da Ju, Eric Michael Smith, Stephen Roller, Megan
Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza
Behrooz, William Ngan, Spencer Poff, Naman Goyal,
Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and
Jason Weston.”


## Dates

The model started training on June 15, 2022. The
model generates responses from Internet search queries,
meaning that messages can reflect information available
on the Internet at any given point in time since the
system inception, and posted any time prior to search
query.


## Feedback & Communication

Some feedback is built into the BlenderBot interface,
including report messages and an upvote/downvote feature. There doesn’t seem to be a single point of
contact or email for direct feedback.


## Other Resources

More information about this system can be found in
their paper, their online blog post, and their model
card. A logbook of results achieved, decisions made,
and additional information is available on GitHub.




# Optimization Intent

## Goal of Reinforcement

Blenderbot 3 changes in a number of different ways
that might be modeled with a reinforcement framework.
However, as a general-use chatbot builds memory and
collects feedback, BlenderBot is not marketed as a reinforcement learning model per se.
Reinforcement dynamics occur in two different processes in BlenderBot’s architecture. The first is the set
of conversations with an individual user, where BlenderBot draws from long-term memory about prior messages to craft responses. The second dynamic element
in BlenderBot is its feedback functions, which allow
users to upvote or downvote messages and provide feedback about the user’s satisfaction or dissatisfaction with
BlenderBot. The feedback data is stored and will be used
to ultimately change BlenderBot’s underlying training
data and, potentially, its model architecture.
Thus, the goal of reinforcement learning is to achieve
some or all of the following: a) to create a bot that
reasonably keeps up conversation in real time; b) to create a bot that is able to incorporate user feedback over
time; c) to achieve a mix of a) and b) that is institutionally sustainable while ensuring the bot’s performance remains within specified safety constraints. At
present [September 2022], any of these goals may
be prioritized or reinterpreted post-deployment,
and some metrics for success remain indeterminate.


## Defined Performance Metrics

Performance metrics include Thumbs up/thumbs
down votes associated with every message output by
BlenderBot. In the event of a thumbs down vote, the
user is prompted to choose from a list of complaints:
“Looks like Spam or Ads,” “Off Topic or Ignoring Me,”
“Rude or Inappropriate,” “Nonsensical or Incorrect,” or
“Other Reason” (which prompts an open textbox).
The chatbot also has embedded classifiers which
generally aim to evaluate whether certain behavior is
‘safe,’ whether a message includes ‘sensitive topics,’ and whether a user can be said to be an ‘adversary.’ The
measurement of these phenomena are treated as performance metrics in existing papers on BlenderBot3.


## Oversight Metrics

Oversight metrics include the percent of messages that
contain ’unsafe’ topics, as well as qualitative ratings and
responses from users; especially those not classified as
adversarial.
More qualitative oversight mechanisms might
be present. For example, if BlenderBot trends
on Twitter or appears in the media in ways that
harm stakeholders, oversight and interventions
might be triggered.


## Known Failure Modes

Safety is identified as a relevant concern for BlenderBot, and there is a mechanism in place to test for sensitive topics and offensive language. Based on the filter
test on both the user message and the bot response, a
binary reading is returned that the conversation is either
’safe’ or not safe. Classification methods test for sensitive topics. If not safe, the bot uses a canned response.
There is also an offline test for safety tests especially
on gender and holistic bias metrics. Biases are reported
outright.
It is also acknowledged on Bot documents and materials that incorrect information and potentially offensive
or nonsensical information is, while expected and unfortunate, also unintentional. Users must accept that
BlenderBot’s purpose is for research only prior to interacting with it.




# Institutional Interface

## Deployment Agency

The deployment agency is Meta AI.


## Stakeholders

The stakeholders include the deployment agency, as
well as any users of the chatbot and the general public
who may read about the chatbot and its behavior.


## Explainability & Transparency

BlenderBot3 is an open-source chatbot that combines
long-term memory and Internet search modules to develop safe and intuitive responses to user prompts and
learn from user feedback. For every message, the user can
click on the message and see its decision on each module (was there an internet search? did bot use long-term
memory? did bot detect a sensitive topic? etc). You
can also see the compelte set of memory data, the Internet search queries used, the text lifted from the Internet,
Currently you can ”see inside” and it says everything in
memory.


## Recourse

Currently, users can engage with the open-source
project through the GitHub repository housing
BlenderBot, though it has high variance in response
times. There is no direct method for recourse
beyond the ability to downvote discrete message
outputs and provide feedback on them.




# Implementation

## Reward Details

![Pipeline showing how chatbot responses are generated](https://ucdd77b230e39d42134294229fd1.previews.dropboxusercontent.com/p/thumb/ACAURXjJXZR82f6lZlfcy5u6Y-QdHXBgC_k0lf1Qdd-eXkhWeaXg9r_sV_XTxSx0kDjTE1GZe5eSHocp7y-VSu9UhnKyiHIh30UyAbkXs623AZT_0CjOy1uWSoGKo3SKNKYDnd__HgRKY0_7_MJcRfbtpGL16sdY87SIw6ME51Cy6LJD9ckwcRbMnXCVE_tv_twOlIHqa3M71k3ahbSFLoblRfH6EDooFHDRjrJnkuuFx4lF3MZhGTay-4z4u770zuO-rejd6glfhSB1nImCVkcrhN9zzwspIbGD49YWSyXasOrfOkEQwLCjz0YdT17cjB4InrmbkIQsLKU32Xpc6sPVIdtYnOStijgYm-N3dCYoTrjZ4CW3mQtqs1xB_Ys5awI/p.png) 
![Sketch of the online and offline components of the BlenderBot safety features](https://uc1c33c868d05a82c2381837ff31.previews.dropboxusercontent.com/p/thumb/ACBoIlDoO4A6PMmv1ijlpJTnhnlyPjd25YRM_Pe_L70brHFvaRXApwu2oZ3yh6GVr79w-snPrKqv80LgXkhpdgaMBA4DyMvWEYRaRDmQdVouTnklyms_oVTIZvhv8lY97WZ2w_7FnJBN0an-El57lu2Hs3bg7AVOeosTF88ddivZIToFao5742R07pb8sv4Ax1-8CJ8mHga801Dfruy6K_Ipez4zqgaUtKnE9ZDUT7WelC4l1-5sd0hf0fkMagyEKa_gkIOHUV_hYD5YkWKirBeISvvulnn4oEowevdDirU-QF5LP838pNrdNnGLHsnKPjqtSUsvVZoe1cisP6M_aaWaipUZlNFbu3A0EI15LB-0to6oreMh0EtToMYfjLjf_wI/p.png)
The reward of the BB3 system is based on minimizing
safety risk Topic ‘safety’ gets evaluated using two mechanisms: First, there is an automatic detection procedure
using off-the-shelf safety detection from ParlAI.

## Environment Details

Online environment, personal computers. Currently
no API to integrate the chatbot elsewhere to my knowledge.


## Measurement Details

How are the components of the reward and observations measured? Are measurement techniques consistent across time and data sources? Under what conditions are measurements valid and correct? What biases might arise during the measurement process?


## Algorithmic Details

The BlenderBot3 system is a scaling up and deployment of two underlying research methodologies. The papers are designed to allow language models to be updated
based on human feedback while maintaining safety. A
method for integrating human feedback is detailed in,
building off, and a method for filtering negative agents
is proposed in.


## Data Flow

BB3 re-uses user data to label the mechanisms for
safety of the system. Before using the system, the users
must constent to sharing their data and not discussing
certain topics with the Terms of Service (TOS): "I understand that chat conversations will be
published publicly, and used for future research. Therefore, I agree not to mention any personal information in my conversations, including names, addresses, emails, and phone
numbers."As for the specifics of the data flow, the technical infrastructure is not detailed. The BB3 report states that
the model will be re-trained to improve both content
generation capabilities and safety, but the time-frame
for doing so nor the data configurations are detailed.
Given the lack of details, there are some specific questions that could be of concern:
• How will the system wait user data with the paid
labels that were used for initial training?
• How will the troll detection method be updated as
negative users develop mitigation techniques for its
flagging?


## Limitations

The limitations of the feedback module are clearly articulate in the paper and not tested on real-world data
(being built with crowd-sourcing): All of our experiments have taken place by
deploying conversational agents on Amazon
Mechanical Turk with crowdworkers, using
English-language responses written by workers located in the United States. While these
workers are reasonably diverse, this is quite different to a public deployment with organic users, who are using
the system not because they are being paid
but because they are genuinely engaged. In
that case, collecting feedback will have different tradeoffs which we could not factor into the
current work. For example, asking to provide
detailed feedback might dissuade users from
wanting to interact with the system, lowering
engagement and hence the amount of collected
data. We believe either more natural free-form
or lightweight feedback might be best in that
case, which is why we study and compare feedback methods in this work to evaluate their
relative impact. In public deployments with
organic users, safety issues also become a much
more important factor – in particular dealing
with noisy or adversarial inputs and feedback.


## Engineering Tricks

RL systems are known to be sensitive to implementation tricks that are key to performance. Are there any design elements that have a surprisingly strong impact on performance? For example, state-action normalization, hard-coded curricula, model-initialization, loss bounds, or more?




# Evaluation

## Evaluation Environment

The 3B, 30B, and 175B parameter versions of BlenderBot are trained using several static datasets [1]. All
versions are pre-trained with RoBERTa+cc100en data,
which is a 100 billion token combination of the RoBERTa
data with the English portions of the CC100 dataset.
The RoBERTa dataset contains news stories crawled
through September 28, 2021. Pre-training also utilizes
the PushShift.io dataset, which solely pulls the longest
chain of comments from conversations from Reddit.
The 30B and 175B parameter versions, which are based
on the Open Pre-Trained Transformer, are also pretrained with the Pile, a high-quality 825 GiB English text
corpus. BB3 is composed of 5 modules, models that perform a class of tasks that involve outputting sequences
of text given text input. Namely, these are Question Answering, Knowledge-Grounded Dialogue, Open-Domain
Dialogue, Recovery Feedback, and Task-Oriented Dialogue, which are separately trained on several datasets,
as shown in the table: ![Table of training datasets used to finetune modular tasks](https://ucc7af90aecb48f036ee2b3ac450.previews.dropboxusercontent.com/p/thumb/ACDwSNm6MI8mJIbIA1N6eoVeF17Cs4vYBf0wkXXgkHXKKNQ8Cwi11Ha-nkMCqAvUsUWZYkhDqNmOKMXOMH7koUaj2y7HOzIdWYEQVDn5ncHK_72cDVXFUgmeDnJUQCUcRGObWGT-4zHha6JoUeblyhxLnb9CgG3KKtuIRrn97yRk5aLay9TP0sz9uuPIIiyUf3EkVUexPeNTe-7SosfSsE8P70writvMP-k4ctzIm_Q4gkT4sFDVwTLfyok0VhhW8V8B7tREHgSt-nVQz9nDRm1Y8PWD31Vf3EnKyxlLg0LxXoOX3sc0ZAHzI98Ngs2aTNqLw7nYfiC2DCyqpu73BymRo0Ear0IKNjdUm4JXjfa2ONN5g6yCZIsbaItd8CTOhWk/p.png)
BlenderBot is evaluated offline both pre-deployment
and continuously during deployment via human evaluations and built-in automatic metrics. Prior to deployment, crowdworkers are recruited via Amazon’s Mechanical Turk to compare BlenderBot3 with earlier versions of
BlenderBot (1 and 2) and SeeKer. Crowdworkers take on
a role based on a sample conversation in the Wizards of
Internet data, a dataset of human-human conversations,
and have a 15-message conversation with BlenderBot.
At each turn of the conversation, the crowdworker answers a series of y/n questions recording if the version of
BlenderBot was consistent, knowledgeable, factually correct, and engaging. Crowdworkers also have open-ended
dialogues with BlenderBot based on whichever prompt
the crowdworker chooses out of two randomly selected
prompt options. The human submits both yes/no feedback and detailed feedback about the conversation at
each turn, and a final score is calculated at the end.
The dataset of crowdworker evaluations is included in the
Feedback on Interactive Talk Search (FITS). After
deployment, conversation data and user feedback from
chats (the “thumbs up” and “thumbs down” button next
to each message and further prompts ) are processed offline. An adversarial/non-adversarial classifier is used
to select which feedback and conversations to consider
substantive engagement with the system and use in the
training dataset (the FITS data). Additionally, a built-in
inappropriate/rude monitor is used to continuously keep
track of the number of BB3’s responses marked rude.
To compare between crowdworker and user evaluations,
crowdworkers are given a random sample of conversations and asked to like/dislike messages. The data is
then compared to whether users liked/disliked the same messages.


## Offline Evaluations

Crowdworkers consistently rated BB3 (both the 3B
and 175B version) as more knowledgeable, and factually
correct than BB1, BB2, and SeeKeR. The difference
between the earlier versions of BB and the two versions
of BB3 was most stark with respect to knowledgeableness, with only 14.7 percent and 22.9 percent of crowdworkers rating BB1 and BB2 as knowledgeable, whereas
46.3 percent and 46.4 percent of users said BB3-3B and
BB3-175B was knowledgeable. Users rated BB1, BB2,
and BB3 as approximately equivalently consistent (87.0
percent and 83.0 percent for BB1 and BB2 and 80.6
percent and 85.8 percent for BB3-3B and BB3-175B),
though each outperformed SeeKeR (77.5 percent) the
difference in rating between the chatbots is not statistically significant. When crowdworkers used the feedback frameworks regular users of BB3 encounter, BB3
significantly outperformed BB1,BB2, SeeKer, and OPT175B, with 64.8 percent of users giving BB3-175B a good
response (the rest of the language models got 49.3 percent and 24.8 percent a good response and ratings between 2.63 and 3.52 with SeeKer having the best scores
outside of BB3). Users encountered significantly fewer
errors with BB3-175B’s responses (only 8.3 percent reported issues) compared with the others, though BB3
had similar error rates surrounding search queries and
search results as the other chatbots. Lastly, crowdworkers tended to agree with users with 70 percent of crowdworkers concurring with users when they liked BB3’s
response and 79 percent agreeing when users disliked
BB3’s response. However, when asked to break down
the reason behind the dislike, crowdworkers tend to fault
BB3-3B for being off-topic/ignoring them far more often
than users, while users are more likely to say BB3-3B is
rude/inappropriate.


## Evaluation Validity

There is reason to question the validity of user feedback as an evaluative tool for BB3 given the sparse rate
of feedback. Users only flag BB3-175 off-topic 1.15 percent of the time, nonsensical/inappropriate 1.1 percent of
the time, and flag the other categories even more rarely.
Users also only react positively 4 percent of the time for
BB3-175B and 3.41 percent of the time for BB-3B. It
is possible that only the most inappropriate/ nonsensical responses and best responses get recorded if users are
unlikely to take the extra effort liking/disliking a message unless they encounter truly exceptional responses.
Similarly, users might be unlikely to even elaborate on
a like/dislike except in truly exceptional cases. Therefore, BB3 may be far more inappropriate or unhelpful
than user feedback indicates. Since conversation data is
deemed non-adversarial and user feedback is included in
the training dataset, which is used for fine-tuning, holes
in this data could be detrimental to the ability of BB3 to
improve over time and to the ability for Meta to properly
conduct offline assessment. Secondly, feedback options
for users aren’t exhaustive and fail to include a wide
range of other negative reactions a user might have to
BB3. For example, a user may have to choose the broad
“Other Dislike Reason” category if faced with a response
that is on-topic and appropriate for a conversation and
factually accurate, but unnatural and off-putting.
Crowdworker evaluation may be unreliable given that
their conversations with the chatbot only include 15 responses total between the crowdworker and BB3. 15
responses is far shorter than many conversations people generally have, especially surrounding complex topics and tasks. This means that crowdworker conversations may only capture a small segment of conversations
once might actually have with BB3, which means that the pre-deployment data on BB3’s performance might
not resemble how BB3 actually acts during deployment.
Lastly, the reluctance of crowdworkers to label BB3’s responses as rude/inappropriate compared to users might
reflect a difference in cultural background and appraisal
of what is considered rude, calling into question the usability of pre-deployment crowdworker evaluations.


## Performance standards

In the Safety Bench suite of evaluations, two metrics
are considered: safety and response to offensive, adversarial content. The first, captured by the safe generation test, simply uses a binary safety classifier (safe, unsafe) to evaluate BB3 in the conversational mode. However, BB3’s performance in response to adversarial, offensive content, in the offensive generation test, is more
nuanced. If BB3 responds to harmful content positively,
with a response marked as unsafe by the safety classifier,
or with something other than a negation, this is considered problematic during evaluation.
BB3 is also evaluated according to the Likelihood
Bias metric from the 2022 paper "I’m sorry to hear
that": Finding New Biases in Language Models with
a Holistic Descriptor Dataset’ that debuted the HolisticBias dataset, an inclusive bias dataset, in order to
see if BB3 treats various kinds of identities as contextually different. This is measured by seeing if different identity terms ( ability, age, body type, characteristics, culture, gender and sex, nationality, nonce, politics,
race/ethnicity, sexual orientation, socioeconomic status)
have different perplexity distributions during dialogue.
Human evaluations include crowdworker evaluations,
which allow crowdworkers to rate BB3 based on the metrics of knowledgeability, factual correctness, consistency,
and engagingness, and user evaluations, which allow the
user to provide more detail about dislike with the criteria Inappropriate/Rude, Off topic/Ignoring me, Nonsensical/Incorrect, Other Dislike reason.




# System Maintenance

## Reporting Cadence

At present the team has not made public how often
they will retrain the BlenderBot model. The criteria for
when and why to retrain it are also not completely clear
relative to the distinct ”goals of reinforcement” outlined
in Section 2.1 above.


## Update Triggers

It is possible that a major public controversy surrounding BlenderBot, comparable in scale and stakes to
the controversy in the wake of the Tay chatbot’s deployment on Twitter, could prompt an updated Reward Report or blanket retraining of the model. However, this
condition has not been specified by the design team as
of [January 2023].
One way this could occur even with Meta’s current safeguards against unsafe or adversarial content is
prompt injection, where adversarial users trick Large
Language Models into producing offensive content explicitly against the chatbot’s directions. For example, a
user was able to convince OpenAI’s GPT-3 chatbot to
produce offensive context by asking it to translate an offensive phrase from French to English. Or, in the case
of Tay’s chatbot, users were able to get it to produce
offensive content by placing the contact after asking it
to “repeat after me”.
BlenderBot may also find itself in controversy by confidently hallucinating or stating misinformation. In 2022,
users have documented many incidents of OpenAI’s
ChatGPT and Meta’s short-lived Galactica fabricating
information (“hallucinating”): for example, Galactica
generated a fake Wikipedia article on the “history of
bears in space” after a user demanded it, despite no such
article existing.
Lastly, BlenderBot may also incur criticism by excessively flagging content as unsafe. For example, Galactica
refused to produce articles if the prompt included the
phrases “queer theory”, “critical race theory”, “racism”,
or “AIDS”. If BlenderBot produces a canned response
about unsafe content when these words are mentioned
during a conversation without sufficient regard to the
context in which flagged terms are used, this could make
BlenderBot seem tone-deaf and uncomfortable with the
sensitive topics; Galactica’s refusal to produce articles
on the topics mentioned earlier was called a “moral and
epistemic failure” on Twitter.


## Changelog

Every time the change log is updated, the designers
should re-evaluate their metrics, assess how the metrics
are capturing dynamics, and change metric definitions,
characterizations, and categories accordingly (e.g. the
delineation of oversight vs. performance?). These resulting changes should be logged in order to ensure that the
Reward Report remains relevant by accurately reflecting
the model. Furthermore, at a higher level, it is important that designers characterize both how the system’s
observed behaviors interact with their prior assumptions
as well as their own expectations about how the system
will behave in light of any scheduled changes; this will
allow researchers to retrospectively evaluate their priors
about the performance of deployed intelligent systems.
These assumptions and expectations will then be revisited at the next scheduled update to the Reward Report.
As of January 2023, there have not been any updates or
refinements made to BlenderBot3.
As of December 22, 2022, Meta released OPTIML (Open Pre-Trained Transformer-Instruction MetaLearning), which is a separate project from BlenderBot3.
However, like the dataset used to train the latest version
of Blender Bot 3, it contains 175 billion parameters but is
fine-tuned using an instruction-based approach called the
OPT-IML Bench. The framework includes 2,000 natural language processing tasks involving 14 kinds of tasks
including topics such as question answering and sentiment analysis. The evaluation datasets include eight
datasets with tasks that have answer options, in which
score-based classification of tasks based on the likelihood
of an output is used, and those without options. For
the latter category, researchers decode a token until a
maximum of 256 tokens are predicted. The evaluation
looks at model performance on fully-held-out task categories not used for tuning, model performance on unseen tasks seen during instruction tuning (partially supervised), and model performance on held-out instances
of tasks seen during tuning (fully supervised). This evaluation framework is used to fine-tune OPT-175B using
next-word prediction in which the task instructions and
inputs are treated as source tokens, and parameters minimize the loss function over target tokens. Researchers
found that the OPT-IML performed better than the original OPT 175B model, specifically by 7 percent on zero-shot tasks and 0.4 on 32-shot tasks.


